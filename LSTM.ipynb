{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing of LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import wandb\n",
    "import pickle\n",
    "import warnings\n",
    "import torch as T\n",
    "from tqdm import tqdm\n",
    "from torch.nn import MSELoss\n",
    "from itertools import compress\n",
    "from utilities.util import colate_fn as colate\n",
    "from model import encoder, generator, smoothing\n",
    "from utilities.util import TrTstSplit, GetInputOutputSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the device and data to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pkls = ['*_graph.pkl'] #insert the name of the processed pkl files\n",
    "losses = 'mse+cosine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the loss function to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pccloss(y, y_hat, lens): #Compute Pearson correlation coefficient\n",
    "    pearson = []\n",
    "    for n in range(lens.shape[0]):\n",
    "        length = lens[n]\n",
    "        # Compute mean of true and predicted values\n",
    "        mean_true = T.mean(y_hat[n,:length,:],dim=0)\n",
    "        mean_pred = T.mean(y[n,:length,:],dim=0)\n",
    "\n",
    "        # Compute Pearson correlation coefficient\n",
    "        numerator = T.sum((y_hat[n,:length,:] - mean_true) * (y[n,:length,:] - mean_pred),dim=0)\n",
    "        denominator = T.sqrt(T.sum((y_hat[n,:length,:] - mean_true)**2,dim=0)) * T.sqrt(T.sum((y[n,:length,:] - mean_pred)**2,dim=0))\n",
    "        correlation_coefficient = numerator / denominator\n",
    "\n",
    "        # Clip correlation coefficient to prevent NaNs in gradients\n",
    "        correlation_coefficient = T.clamp(correlation_coefficient, -1.0, 1.0)\n",
    "\n",
    "        # Convert correlation coefficient to correlation loss (1 - r)\n",
    "        correlation_loss = 1.0 - correlation_coefficient\n",
    "        pearson.append(T.mean(correlation_loss))\n",
    "    return T.mean(T.tensor(pearson))\n",
    "\n",
    "def lossmse(y, y_hat, lens):    #Compute mean squared error\n",
    "    mse = T.div(T.sum(T.square(y-y_hat), dim=[1,2]), lens)\n",
    "    return T.mean(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the models to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the lstm and gnn models \n",
    "LSTM_model = T.nn.Sequential(encoder.LSTM(512, 256, 2),\n",
    "                             generator.LSTM(256, 2, 3),\n",
    "                             smoothing.LearnableGaussianSmoothing(5)).to(device)\n",
    "GCN_model = T.nn.Sequential(encoder.GCN(512, 256),\n",
    "                            generator.GCN(256, 3),\n",
    "                            smoothing.LearnableGaussianSmoothing(5)).to(device)\n",
    "models = {'LSTM':LSTM_model,'graph':GCN_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dirs = glob.glob('processed_data/'+pkls[0]) #insert the name of the processed pkl files instead of pkls[0]\n",
    "input = []\n",
    "output = []\n",
    "for cv in range(len(dirs)):\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in range(len(dirs)):\n",
    "        with open(dirs[i], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        if 'Session2' in dirs[i]:   #Remove the columns that are not processed properly\n",
    "            for c in [41, 62, 69, 69, 73, 80, 100, 122, 150, 154, 374]:\n",
    "                del data[c]\n",
    "        if i == cv:\n",
    "            test = data\n",
    "        else:\n",
    "            train.extend(data)\n",
    "    model = models['LSTM']\n",
    "    optimizer = T.optim.Adam(model.parameters(), lr=0.01)       #Optimizer and Scheduler \n",
    "    scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1,\n",
    "                        patience=50, threshold=0.01, threshold_mode='rel',\n",
    "                        verbose=True)\n",
    "\n",
    "    dataloader = T.utils.data.DataLoader(train, batch_size=64, num_workers=4, collate_fn=colate)    #Training Dataloader\n",
    "    dataloaderT = T.utils.data.DataLoader(test, batch_size=1)   #Testing Dataloader\n",
    "    Alpha = T.sigmoid(T.nn.Parameter(data=T.Tensor(1), requires_grad=True))\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_pcc = 0\n",
    "    epoch=0\n",
    "    epochs=tqdm(total = 500)\n",
    "    while epoch<500 and optimizer.param_groups[0]['lr']>0.000001:       #Training loop with 500 epochs limit and learning rate limit\n",
    "        epochs.__iter__()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_loss_pcc = 0.0\n",
    "        for X, _, hea, lens, _, _ in dataloader:\n",
    "            X = X.squeeze()\n",
    "            X = X.to(device)\n",
    "            hea = T.squeeze(hea).to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with T.set_grad_enabled(True):\n",
    "                outputs = model(X)\n",
    "                loss = lossmse(outputs, hea, T.tensor(lens).to(device))\n",
    "                loss2 = pccloss(hea, outputs, T.tensor(lens).to(device))\n",
    "                (Alpha*loss + (T.Tensor(1)-Alpha)*loss2).backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item()\n",
    "            running_loss_pcc += loss2.item()\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_loss_pcc = running_loss_pcc / len(dataloader)\n",
    "        scheduler.step(epoch_loss + epoch_loss_pcc)\n",
    "        epoch+=1\n",
    "        epochs.update(1)\n",
    "        epochs.set_postfix(mse_loss=epoch_loss, pcc_loss=epoch_loss_pcc)        #Print the iteration loss values\n",
    "        \n",
    "    \n",
    "    model.eval()\n",
    "    for audio, _, pose, _, _ in dataloaderT:    #Testing loop\n",
    "        audio = T.squeeze(audio)\n",
    "        audio = audio.unsqueeze(0)\n",
    "        audio = audio.to(device)\n",
    "        pose = pose.to(device)\n",
    "        outputs = model(audio)\n",
    "        input.append(pose.detach().cpu().numpy())\n",
    "        output.append(outputs.detach().cpu().numpy())\n",
    "dump = {'name':'LSTM','input':input,'output':output}    #Save the testing outputs\n",
    "with open('results/'+'LSTM_learn.pkl', 'wb') as f:\n",
    "    pickle.dump(dump, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backchannel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
