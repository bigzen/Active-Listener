{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import wandb\n",
    "#import pysdtw\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch as T\n",
    "from tqdm import tqdm\n",
    "from torch.nn import MSELoss\n",
    "from itertools import compress\n",
    "from utilities.multi_loss import MultiTaskLoss\n",
    "from utilities.util import colate_fn as colate\n",
    "from model import encoder, generator, smoothing\n",
    "from utilities.util import TrTstSplit, GetInputOutputSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pkls = ['*_lstm.pkl', '*_graph.pkl', '*_graph_mfc.pkl', '*_graph_pitch.pkl', '*_graph_eg.pkl']\n",
    "losses = 'mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the lstm and gnn models \n",
    "LSTM_model = T.nn.Sequential(encoder.LSTM(29, 256, 2),\n",
    "                             generator.LSTM(256, 2, 3),\n",
    "                             smoothing.LearnableGaussianSmoothing(5)).to(device)\n",
    "GCN_model = T.nn.Sequential(encoder.GCN(88, 256),\n",
    "                            generator.GCN(256, 3),\n",
    "                            smoothing.LearnableGaussianSmoothing(5)).to(device)\n",
    "models = {'LSTM':LSTM_model,'graph':GCN_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pcc(y, y_hat):\n",
    "    #print(y.shape,y_hat.shape)\n",
    "    mean_true = T.mean(y,dim=0)\n",
    "    mean_pred = T.mean(y_hat,dim=0)\n",
    "\n",
    "    # Compute Pearson correlation coefficient\n",
    "    numerator = T.sum((y_hat - mean_pred) * (y - mean_true),dim=0)\n",
    "    denominator = T.sqrt(T.sum((y_hat - mean_pred)**2,dim=0)) * T.sqrt(T.sum((y - mean_true)**2,dim=0))\n",
    "    correlation_coefficient = numerator / denominator\n",
    "\n",
    "    # Clip correlation coefficient to prevent NaNs in gradients\n",
    "    correlation_coefficient = T.clamp(correlation_coefficient, -1.0, 1.0)\n",
    "\n",
    "    # Convert correlation coefficient to correlation loss (1 - r)\n",
    "    correlation_loss = 1.0 - correlation_coefficient\n",
    "    return T.mean(correlation_loss)\n",
    "\n",
    "def pccloss(y, y_hat, lens):\n",
    "    #print(y.shape,y_hat.shape)\n",
    "    pearson = []\n",
    "    \n",
    "    curr = -1\n",
    "    while True:\n",
    "        #print(lens[0,curr])\n",
    "        if lens[0,curr]==0:\n",
    "            break\n",
    "        if curr==-1:\n",
    "            correlation_loss = compute_pcc(y[lens[0,curr]:,:], y_hat[lens[0,curr]:,:])\n",
    "        else:\n",
    "            correlation_loss = compute_pcc(y[lens[0,curr]:curr+1,:], y_hat[lens[0,curr]:curr+1,:])\n",
    "        curr = lens[0,curr]-1\n",
    "        pearson.append(correlation_loss)\n",
    "    loss = T.mean(T.tensor(pearson))\n",
    "    loss.requires_grad=True\n",
    "    return loss\n",
    "\n",
    "def cosineloss(y, y_hat, lens):\n",
    "    #print(y.shape,y_hat.shape)\n",
    "    pearson = []\n",
    "    \n",
    "    curr = -1\n",
    "    while True:\n",
    "        #print(lens[0,curr])\n",
    "        if lens[0,curr]==0:\n",
    "            break\n",
    "        if curr==-1:\n",
    "            correlation_loss = compute_cosine(y[lens[0,curr]:,:], y_hat[lens[0,curr]:,:])\n",
    "        else:\n",
    "            correlation_loss = compute_cosine(y[lens[0,curr]:curr+1,:], y_hat[lens[0,curr]:curr+1,:])\n",
    "        curr = lens[0,curr]-1\n",
    "        pearson.append(correlation_loss)\n",
    "    loss = T.mean(T.tensor(pearson))\n",
    "    loss.requires_grad=True\n",
    "    return loss\n",
    "\n",
    "def compute_cosine(y, y_hat):\n",
    "    cos = T.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    cosine = cos(y, y_hat)\n",
    "    return 1-T.mean(cosine)\n",
    "\n",
    "def lossmse(y, y_hat):\n",
    "    mse = MSELoss()\n",
    "    \n",
    "    return mse(y,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [15:07<00:00,  1.81s/it, mse_loss=72.5]\n",
      "100%|██████████| 500/500 [15:02<00:00,  1.80s/it, mse_loss=92]\n",
      "100%|██████████| 500/500 [15:19<00:00,  1.84s/it, mse_loss=70.6]\n",
      "100%|██████████| 500/500 [16:17<00:00,  1.96s/it, mse_loss=48.3]\n",
      "100%|██████████| 500/500 [15:23<00:00,  1.82s/it, mse_loss=39.8]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch_geometric as pyg\n",
    "dirs = glob.glob('processed_data/'+pkls[4])\n",
    "input = []\n",
    "output = []\n",
    "for cv in range(len(dirs)):\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in range(len(dirs)):\n",
    "        with open(dirs[i], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        if 'Session2' in dirs[i]:\n",
    "            for c in [41, 62, 69, 69, 73, 80, 100, 122, 150, 154, 374]:\n",
    "                del data[c]\n",
    "        if i == cv:\n",
    "            test = data\n",
    "        else:\n",
    "            train.extend(data)\n",
    "    model = models['graph']\n",
    "    optimizer = T.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5,\n",
    "                        patience=50, threshold=0.01, threshold_mode='rel',\n",
    "                        verbose=True)\n",
    "\n",
    "    dataloader = pyg.loader.DataLoader(train, batch_size=64, num_workers=4)\n",
    "    dataloaderT = pyg.loader.DataLoader(test, batch_size=1)\n",
    "    combiloss = MultiTaskLoss()\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_pcc = 0\n",
    "    epoch=0\n",
    "    epochs=tqdm(total = 500)\n",
    "    while epoch<500 and optimizer.param_groups[0]['lr']>0.000001:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_loss_pcc = 0.0\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with T.set_grad_enabled(True):\n",
    "                outputs = model([data.x.to(device), data.edge_index.to(device)])\n",
    "                loss = lossmse(outputs, data.y.to(device))\n",
    "                #loss2 = cosineloss(data.y.to(device), outputs.squeeze(), data.edge_index.to(device))\n",
    "                #loss = combiloss(loss,loss2)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item()\n",
    "            #running_loss_pcc += loss2.item()\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        #epoch_loss_pcc = running_loss_pcc / len(dataloader)\n",
    "        scheduler.step(epoch_loss)# + epoch_loss_pcc)\n",
    "        epoch+=1\n",
    "        epochs.update(1)\n",
    "        epochs.set_postfix(mse_loss=epoch_loss)#, pcc_loss=epoch_loss_pcc )\n",
    "        \n",
    "    \n",
    "    model.eval()\n",
    "    for data in dataloaderT:\n",
    "        outputs = model([data.x.to(device), data.edge_index.to(device)])\n",
    "        input.append(data.y.detach().cpu().numpy())\n",
    "        output.append(outputs.detach().cpu().numpy())\n",
    "dump = {'name':'graph_eg_mse','input':input,'output':output}\n",
    "with open('results/'+'graph_eg_mse.pkl', 'wb') as f:\n",
    "    pickle.dump(dump, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for Graph Inter Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7006, 3]), torch.Size([0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape, data.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../IEMOCAP_full_release/'\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3320211 1423.9486234789726\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import torch_geometric as pyg\n",
    "from utilities.graphData import MocapDataset as MocapDataset\n",
    "GCN_model = T.nn.Sequential(encoder.GCN(512, 256),\n",
    "                            generator.GCN(256, 3),\n",
    "                            smoothing.LearnableGaussianSmoothing(5)).to(device)\n",
    "param_count = sum(p.numel() for p in GCN_model.parameters() if p.requires_grad)\n",
    "len=0\n",
    "dirs = glob.glob('*.csv')\n",
    "head, aud, landM, emo, val = GetInputOutputSplit([dirs[1]])\n",
    "dataset = MocapDataset(head, aud, landM, emo, val)\n",
    "dataloader = pyg.loader.DataLoader(dataset, batch_size=1 )\n",
    "GCN_model.eval()\n",
    "start = time()\n",
    "for data in dataloader:\n",
    "    outputs = GCN_model([data.x.to(device), data.edge_index.to(device)])\n",
    "    len+=data.x.shape[0]\n",
    "tick = time()-start\n",
    "print(param_count,len/tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384.4568916184305"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len/tick"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backchannel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
